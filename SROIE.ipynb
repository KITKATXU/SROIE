{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning LayoutLM using the SROIE dataset\n\nThis is my first attempt to fine-tune the LayoutLM model.\n\nI used base codes from:https://www.kaggle.com/code/urbikn/layoutlm-using-the-sroie-dataset\n\nI improved the model performance by\n\n1)k-fold cross-validation\n2)improving the possibility of labeling S-TOTAL","metadata":{}},{"cell_type":"markdown","source":"# 1. Pre-processing the dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport json \nimport random\nfrom pathlib import Path\nfrom difflib import SequenceMatcher\n\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom IPython.display import display\nimport matplotlib\nfrom matplotlib import pyplot, patches","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sroie_folder_path = Path('/kaggle/input/sroie-datasetv2/SROIE2019')\nexample_file = Path('X51005365187.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_bbox_and_words(path: Path):\n  bbox_and_words_list = []\n\n  with open(path, 'r', errors='ignore') as f:\n    for line in f.read().splitlines():\n      if len(line) == 0:\n        continue\n        \n      split_lines = line.split(\",\")\n\n      bbox = np.array(split_lines[0:8], dtype=np.int32)\n      text = \",\".join(split_lines[8:])\n\n      # From the splited line we save (filename, [bounding box points], text line).\n      # The filename will be useful in the future\n      bbox_and_words_list.append([path.stem, *bbox, text])\n    \n  dataframe = pd.DataFrame(bbox_and_words_list, columns=['filename', 'x0', 'y0', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'line'], dtype=np.int16)\n  dataframe = dataframe.drop(columns=['x1', 'y1', 'x3', 'y3'])\n\n  return dataframe\n\n\n# Example usage\nbbox_file_path = sroie_folder_path / \"test/box\" / example_file\nprint(\"== File content ==\")\n!head -n 5 \"{bbox_file_path}\"\n\nbbox = read_bbox_and_words(path=bbox_file_path)\nprint(\"\\n== Dataframe ==\")\nbbox.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_entities(path: Path):\n  with open(path, 'r') as f:\n    data = json.load(f)\n\n  dataframe = pd.DataFrame([data])\n  return dataframe\n\n\n# Example usage\nentities_file_path = sroie_folder_path /  \"test/entities\" / example_file\nprint(\"== File content ==\")\n!head \"{entities_file_path}\"\n\nentities = read_entities(path=entities_file_path)\nprint(\"\\n\\n== Dataframe ==\")\nentities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign a label to the line by checking the similarity\n# of the line and all the entities\ndef assign_line_label(line: str, entities: pd.DataFrame):\n    line_set = line.replace(\",\", \"\").strip().split()\n    for i, column in enumerate(entities):\n        entity_values = entities.iloc[0, i].replace(\",\", \"\").strip()\n        entity_set = entity_values.split()\n        \n        \n        matches_count = 0\n        for l in line_set:\n            if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n                matches_count += 1\n            \n            if (column.upper() == 'ADDRESS' and (matches_count / len(line_set)) >= 0.5) or \\\n               (column.upper() != 'ADDRESS' and (matches_count == len(line_set))) or \\\n               matches_count == len(entity_set):\n                return column.upper()\n\n    return \"O\"\n\n\nline = bbox.loc[1,\"line\"]\nlabel = assign_line_label(line, entities)\nprint(\"Line:\", line)\nprint(\"Assigned label:\", label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assign_labels(words: pd.DataFrame, entities: pd.DataFrame):\n    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n    already_labeled = {\"TOTAL\": False,\n                       \"DATE\": False,\n                       \"ADDRESS\": False,\n                       \"COMPANY\": False,\n                       \"O\": False\n    }\n\n    # Go through every line in $words and assign it a label\n    labels = []\n    for i, line in enumerate(words['line']):\n        label = assign_line_label(line, entities)\n\n        already_labeled[label] = True\n        if (label == \"ADDRESS\" and already_labeled[\"TOTAL\"]) or \\\n           (label == \"COMPANY\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"])):\n            label = \"O\"\n\n        # Assign to the largest bounding box\n        if label in [\"TOTAL\", \"DATE\"]:\n            x0_loc = words.columns.get_loc(\"x0\")\n            bbox = words.iloc[i, x0_loc:x0_loc+4].to_list()\n            area = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n\n            if max_area[label][0] < area:\n                max_area[label] = (area, i)\n\n            label = \"O\"\n\n        labels.append(label)\n\n    labels[max_area[\"DATE\"][1]] = \"DATE\"\n    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n\n    words[\"label\"] = labels\n    return words\n\n\n# Example usage\nbbox_labeled = assign_labels(bbox, entities)\nbbox_labeled.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_line(line: pd.Series):\n  line_copy = line.copy()\n\n  line_str = line_copy.loc[\"line\"]\n  words = line_str.split(\" \")\n\n  # Filter unwanted tokens\n  words = [word for word in words if len(word) >= 1]\n\n  x0, y0, x2, y2 = line_copy.loc[['x0', 'y0', 'x2', 'y2']]\n  bbox_width = x2 - x0\n  \n\n  new_lines = []\n  for index, word in enumerate(words):\n    x2 = x0 + int(bbox_width * len(word)/len(line_str))\n    line_copy.at['x0', 'x2', 'line'] = [x0, x2, word]\n    new_lines.append(line_copy.to_list())\n    x0 = x2 + 5 \n\n  return new_lines\n\n\n# Example usage\nnew_lines = split_line(bbox_labeled.loc[1])\nprint(\"Original row:\")\ndisplay(bbox_labeled.loc[1:1,:])\n\nprint(\"Splitted row:\")\npd.DataFrame(new_lines, columns=bbox_labeled.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import perf_counter\ndef dataset_creator(folder: Path):\n  bbox_folder = folder / 'box'\n  entities_folder = folder / 'entities'\n  img_folder = folder / 'img'\n\n  # Sort by filename so that when zipping them together\n  # we don't get some other file (just in case)\n  entities_files = sorted(entities_folder.glob(\"*.txt\"))\n  bbox_files = sorted(bbox_folder.glob(\"*.txt\"))\n  img_files = sorted(img_folder.glob(\"*.jpg\"))\n\n  data = []\n\n  print(\"Reading dataset:\")\n  for bbox_file, entities_file, img_file in tqdm(zip(bbox_files, entities_files, img_files), total=len(bbox_files)):            \n    # Read the files\n    bbox = read_bbox_and_words(bbox_file)\n    entities = read_entities(entities_file)\n    image = Image.open(img_file)\n\n    # Assign labels to lines in bbox using entities\n    bbox_labeled = assign_labels(bbox, entities)\n    del bbox\n\n    # Split lines into separate tokens\n    new_bbox_l = []\n    for index, row in bbox_labeled.iterrows():\n      new_bbox_l += split_line(row)\n    new_bbox = pd.DataFrame(new_bbox_l, columns=bbox_labeled.columns, dtype=np.int16)\n    del bbox_labeled\n\n\n    # Do another label assignment to keep the labeling more precise \n    for index, row in new_bbox.iterrows():\n      label = row['label']\n\n      if label != \"O\":\n        entity_values = entities.iloc[0, entities.columns.get_loc(label.lower())]\n        entity_set = entity_values.split()\n        \n        if any(SequenceMatcher(a=row['line'], b=b).ratio() > 0.7 for b in entity_set):\n            label = \"S-\" + label\n        else:\n            label = \"O\"\n      \n      new_bbox.at[index, 'label'] = label\n\n    width, height = image.size\n  \n    data.append([new_bbox, width, height])\n\n  return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset_creator(sroie_folder_path / 'train')\ndataset_test = dataset_creator(sroie_folder_path / 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize(points: list, width: int, height: int) -> list:\n  x0, y0, x2, y2 = [int(p) for p in points]\n  \n  x0 = int(1000 * (x0 / width))\n  x2 = int(1000 * (x2 / width))\n  y0 = int(1000 * (y0 / height))\n  y2 = int(1000 * (y2 / height))\n\n  return [x0, y0, x2, y2]\n\n\ndef write_dataset(dataset: list, output_dir: Path, name: str):\n  print(f\"Writing {name}ing dataset:\")\n  with open(output_dir / f\"{name}.txt\", \"w+\", encoding=\"utf8\") as file, \\\n       open(output_dir / f\"{name}_box.txt\", \"w+\", encoding=\"utf8\") as file_bbox, \\\n       open(output_dir / f\"{name}_image.txt\", \"w+\", encoding=\"utf8\") as file_image:\n\n      # Go through each dataset\n      for datas in tqdm(dataset, total=len(dataset)):\n        data, width, height = datas\n        \n        filename = data.iloc[0, data.columns.get_loc('filename')]\n\n        # Go through every row in dataset\n        for index, row in data.iterrows():\n          bbox = [int(p) for p in row[['x0', 'y0', 'x2', 'y2']]]\n          normalized_bbox = normalize(bbox, width, height)\n\n          file.write(\"{}\\t{}\\n\".format(row['line'], row['label']))\n          file_bbox.write(\"{}\\t{} {} {} {}\\n\".format(row['line'], *normalized_bbox))\n          file_image.write(\"{}\\t{} {} {} {}\\t{} {}\\t{}\\n\".format(row['line'], *bbox, width, height, filename))\n\n        # Write a second newline to separate dataset from others\n        file.write(\"\\n\")\n        file_bbox.write(\"\\n\")\n        file_image.write(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_directory = Path('/kaggle/working','dataset')\n\ndataset_directory.mkdir(parents=True, exist_ok=True)\n\nwrite_dataset(dataset_train, dataset_directory, 'train')\nwrite_dataset(dataset_test, dataset_directory, 'test')\n\n# Creating the 'labels.txt' file to the the model what categories to predict.\nlabels = ['COMPANY', 'DATE', 'ADDRESS', 'TOTAL']\nIOB_tags = ['S']\nwith open(dataset_directory / 'labels.txt', 'w') as f:\n  for tag in IOB_tags:\n    for label in labels:\n      f.write(f\"{tag}-{label}\\n\")\n  # Writes in the last label O - meant for all non labeled words\n  f.write(\"O\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tra=open('/kaggle/working/dataset/train.txt','r').readlines()\ntrabox=open('/kaggle/working/dataset/train_box.txt','r').readlines()\ntraimage=open('/kaggle/working/dataset/train_image.txt','r').readlines()\n\ntst=open('/kaggle/working/dataset/test.txt','r').readlines()\ntstbox=open('/kaggle/working/dataset/test_box.txt','r').readlines()\ntstimage=open('/kaggle/working/dataset/test_image.txt','r').readlines()\n\nnew_train=open('/kaggle/working/dataset/train_new.txt','w')\nnew_train_box=open('/kaggle/working/dataset/train_box_new.txt','w')\nnew_train_image=open('/kaggle/working/dataset/train_image_new.txt','w')\n\ntra.extend(tst)\ntrabox.extend(tstbox)\ntraimage.extend(tstimage)\n\n\n\nfor i, j, k in zip(tra,trabox,traimage):\n    new_train.write(i)\n    new_train_box.write(j)\n    new_train_image.write(k)\n\nprint(len(tra),len(trabox),len(traimage))\nprint(tra[113043],trabox[113043],traimage[113043])\ntmp=open('/kaggle/working/dataset/train_box_new.txt','r').readlines()\nlen(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cat /kaggle/working/dataset/train_new.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp=open('/kaggle/working/dataset/train_new.txt','r').readlines()\nlen(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/dataset/train.txt /kaggle/working/dataset/train1.txt \n!mv /kaggle/working/dataset/train_box.txt /kaggle/working/dataset/train1_box.txt \n!mv /kaggle/working/dataset/train_image.txt /kaggle/working/dataset/train1_image.txt ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/dataset/train_new.txt /kaggle/working/dataset/train.txt \n!mv /kaggle/working/dataset/train_box_new.txt /kaggle/working/dataset/train_box.txt \n!mv /kaggle/working/dataset/train_image_new.txt /kaggle/working/dataset/train_image.txt ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp=open('/kaggle/working/dataset/train_box.txt','r').readlines()\nlen(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Fine tune LayoutLM\n1)downloaded and transformed our dataset into a trainable and testable set\n\n2)clone the LayoutLM Github project which contains the script to fine tune the model.","metadata":{}},{"cell_type":"code","source":"%%bash\ngit clone https://github.com/microsoft/unilm.git\ncd unilm/layoutlm/deprecated\npip install .","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split training data based on Kfold from sklearn","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/dataset/trainfold1\n!mkdir /kaggle/working/dataset/trainfold2\n!mkdir /kaggle/working/dataset/trainfold3\n!mkdir /kaggle/working/dataset/trainfold4\n!mkdir /kaggle/working/dataset/trainfold5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/dataset/trainmodel1\n!mkdir /kaggle/working/dataset/trainmodel2\n!mkdir /kaggle/working/dataset/trainmodel3\n!mkdir /kaggle/working/dataset/trainmodel4\n!mkdir /kaggle/working/dataset/trainmodel5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ximage=open('/kaggle/working/dataset/train_image.txt','r').readlines()\ndict_files={}\ndict_len={}\nct=0\nfor i in Ximage:\n    if i=='\\n':\n        ct=ct+1\n        continue\n    i=i.strip('\\n')\n    i=i.split('\\t')\n    if i[-1] not in dict_files:\n        dict_files[i[-1]]=[]\n        dict_files[i[-1]].append(ct)\n        dict_len[i[-1]]=1\n    else:\n        dict_len[i[-1]]=dict_len[i[-1]]+1\n        dict_files[i[-1]]=dict_files[i[-1]][:1]\n        dict_files[i[-1]].append(ct)\n    ct=ct+1\nlen(dict_files)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ct=0\nfor i in dict_files.keys():\n    ct=ct+1\n    print (i,dict_files[i])\n    if ct>3:\n        break\nct=0\nfor i in dict_len.keys():\n    ct=ct+1\n    print (i,dict_len[i])\n    if ct>3:\n        break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold  # 从sklearn导入KFold包\n\nkf = KFold(n_splits=5,random_state=1211,shuffle=True)\nkf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_=[]\n\nfor i in dict_files.keys():\n    tmp=dict_files[i]\n    list_.append(tmp)\nnp_files=np.array(list_)\nnp_files\n# ct=0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ct=0\n# for X_train,X_test in kf.split(np_files):\n#     ct=ct+1\n#     print(X_train,X_test)\n#     W1=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train.txt','w')\n#     W2=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train_box.txt','w')\n#     W3=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train_image.txt','w')\n# #     for i,j,k in zip(X[int(X_train[0]):int(X_train[-1])],Xbox[int(X_train[0]):int(X_train[-1])],Ximage[int(X_train[0]):int(X_train[-1])]):\n# #         print(ct)\n#     for i in range(len(X_train)):\n#         for j in range(np_files[int(X_train[i])][0],np_files[int(X_train[i])][-1]+2)\n#         W1.write(X[j])\n#         W2.write(Xbox[j])\n#         W3.write(Ximage[j])\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nX=open('/kaggle/working/dataset/train.txt','r').readlines()\nX_np = np.array(X) \n\nXbox=open('/kaggle/working/dataset/train_box.txt','r').readlines()\nXbox_np = np.array(Xbox) \n\nXimage=open('/kaggle/working/dataset/train_image.txt','r').readlines()\nXimage_np = np.array(Ximage) \nlen(X_np),len(Xbox_np),len(Ximage_np)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ct=0\nfor X_train,X_test in kf.split(np_files):\n    ct=ct+1\n#     print(X_train,X_test)\n    W1=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train.txt','w')\n    W2=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train_box.txt','w')\n    W3=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train_image.txt','w')\n#     for i,j,k in zip(X[int(X_train[0]):int(X_train[-1])],Xbox[int(X_train[0]):int(X_train[-1])],Ximage[int(X_train[0]):int(X_train[-1])]):\n#         print(ct)\n    for i in range(len(X_train)):\n        for j in range(np_files[int(X_train[i])][0],np_files[int(X_train[i])][-1]+2):\n          W1.write(X[j])\n          W2.write(Xbox[j])\n          W3.write(Ximage[j])\nW1.close()\nW2.close()\nW3.close()\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=open('/kaggle/working/dataset/trainfold1/train.txt','r').readlines()\nX_np = np.array(X) \n\nXbox=open('/kaggle/working/dataset/trainfold2/train_box.txt','r').readlines()\nXbox_np = np.array(Xbox) \n\nXimage=open('/kaggle/working/dataset/trainfold3/train_image.txt','r').readlines()\nXimage_np = np.array(Ximage) \n# len(X_np),len(Xbox_np),len(Ximage_np)\nX_np[82:90],Xbox_np[82:90],Ximage_np[82:90]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ct=0\n# for X_train,X_test in kf.split(X_np):\n#     print(X_train,X_test)\n#     ct=ct+1\n#     W1=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train.txt','w')\n#     W2=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train_box.txt','w')\n#     W3=open('/kaggle/working/dataset/trainfold'+str(ct)+'/train_image.txt','w')\n# #     for i,j,k in zip(X[int(X_train[0]):int(X_train[-1])],Xbox[int(X_train[0]):int(X_train[-1])],Ximage[int(X_train[0]):int(X_train[-1])]):\n# #         print(ct)\n#     for i in range(len(X_train)):\n#         W1.write(X[int(X_train[i])])\n#         W2.write(Xbox[int(X_train[i])])\n#         W3.write(Ximage[int(X_train[i])])\n# #     print()\n# # len(X[int(X_train[0]):int(X_train[-1])])\n# # type(int(X_train[0]))\n\n# #     print(X_train,X_test)\n      \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp=open('/kaggle/working/dataset/trainfold1/train.txt','r').readlines()\nlen(tp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../../../../../../input/tst111/test.txt /kaggle/working/dataset\n!cp ../../../../../../input/tst111/test_box.txt /kaggle/working/dataset\n!cp ../../../../../../input/tst111/test_image.txt /kaggle/working/dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../../../../../../input/runseqr/run_seq_labeling_r.py /kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling\n!cp ../../../../../../input/d/qingxiaoxu/runseq/run_seq_labeling.py /kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/dataset/cached*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/dataset/trainfold1/cached*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained_model_folder_input= sroie_folder_path / Path('layoutlm-base-uncased') # Define it so we can copy it into our working directory\n\n# pretrained_model_folder=Path('/kaggle/working/layoutlm-base-uncased/') \nlabel_file=Path(dataset_directory, \"labels.txt\")\n\n# Move to the script directory\nos.chdir(\"/kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cp -r \"{pretrained_model_folder}\"/* /kaggle/working/dataset/trainmodel5\n! cp -r \"{pretrained_model_folder_input}\"/* /kaggle/working/dataset/trainmodel5\n\n! sed -i 's/\"num_attention_heads\": 16,/\"num_attention_heads\": 12,/' /kaggle/working/dataset/trainmodel5/config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls /kaggle/working/dataset/trainfold5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cat /kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling/pred.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nThe following steps should be repeated 5 times loading data from /kaggle/working/dataset/trainfold1-5","metadata":{}},{"cell_type":"code","source":"! rm -rf /kaggle/working/dataset/trainfold5/cached*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls /kaggle/working/dataset/trainmodel5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#current5\n! python run_seq_labeling_r.py \\\n                            --data_dir /kaggle/working/dataset/trainfold5 \\\n                            --labels /kaggle/working/dataset/labels.txt \\\n                            --model_name_or_path /kaggle/working/dataset/trainmodel5 \\\n                            --model_type layoutlm \\\n                            --max_seq_length 512 \\\n                            --do_lower_case \\\n                            --do_train \\\n                            --num_train_epochs 11 \\\n                            --logging_steps 50 \\\n                            --save_steps -1 \\\n                            --output_dir output \\\n                            --overwrite_output_dir \\\n                            --per_gpu_train_batch_size 8 \\\n                            --per_gpu_eval_batch_size 16","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls /kaggle/working/dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting","metadata":{}},{"cell_type":"code","source":"!cp ../../../../../../input/d/qingxiaoxu/runseq/run_seq_labeling.py /kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling/run_seq_labeling.py  /kaggle/working/unilm/layoutlm/deprecated/examples/seq_labeling/run_seq_labeling1.py ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/dataset/cached*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls /kaggle/working/dataset/ ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate for test set and make predictions\n#current fold5\n! python run_seq_labeling.py \\\n                            --data_dir /kaggle/working/dataset \\\n                            --labels /kaggle/working/dataset/labels.txt \\\n                            --model_name_or_path /kaggle/working/dataset/trainmodel5 \\\n                            --model_type layoutlm \\\n                            --do_lower_case \\\n                            --max_seq_length 512 \\\n                            --do_predict \\\n                            --logging_steps 10 \\\n                            --save_steps -1 \\\n                            --output_dir output \\\n                            --per_gpu_eval_batch_size 8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_label=np.array(np.load('out_label_ids.npy'))\nct=0\ntp=[]\nfor i in out_label[-6]:\n      ct=ct+1\n      if i>0: \n        tp.append(ct)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv datasum.npy o_datasum.npy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv datasumnew.npy datasum.npy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I found the S-TOTAL label F1 extremly low. However, one of the five models trained from the folds are capable of detecting the actual label of S-TOTAL. \n\nI simply added 20 on those tokens that are of much higher possibility being identified as S-TOTAL than the first 3 labels. ","metadata":{}},{"cell_type":"code","source":"data1=np.array(np.load('save_preds1.npy'))\ndata2=np.array(np.load('save_preds2.npy'))\ndata3=np.array(np.load('save_preds3.npy'))\ndata4=np.array(np.load('save_preds4.npy'))\ndata5=np.array(np.load('save_preds5.npy'))\ndatasum=np.array(np.load('datasum.npy'))\n# len(data1),len(data1[0])\nfor i_ds in range(len(datasum)):\n    for i in range(len(tp)):\n      if datasum[i_ds][tp[i]][-2]>10:\n        datasum[i_ds][tp[i]][-2]=datasum[i_ds][tp[i]][-2]+20\n\nnp.save('datasumnew.npy',datasum)\n#         print(i,data1[-6][tp[i]],'\\n',i,data2[-6][tp[i]])\n#         print(i,data3[-6][tp[i]],'\\n',i,data4[-6][tp[i]])\n#         print(i,data5[-6][tp[i]],'\\n',i,datasum[-6][tp[i]])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf save_preds1.npy\n# !rm -rf save_preds2.npy\n# !rm -rf save_preds3.npy\n# !rm -rf save_preds4.npy\n# !rm -rf save_preds5.npy\n# !rm -rf datasum.npy\n# !rm -rf save_preds.npy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv save_preds.npy save_preds5.npy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1=np.array(np.load('save_preds1.npy'))\ndata2=np.array(np.load('save_preds2.npy'))\ndata3=np.array(np.load('save_preds3.npy'))\ndata4=np.array(np.load('save_preds4.npy'))\ndata5=np.array(np.load('save_preds5.npy'))\ndata_sum =data1+data2+data3+data4+data5\n# data_sum =data1+data2+data3+data4+data5\nnp.save('datasum.npy',data_sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp output/test_predictions.txt /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}